{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_ROOT = 'https://fr.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_THRESHOLD = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = set(['ville', 'transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get(url):\n",
    "    if url in CACHE:\n",
    "        return CACHE[url]\n",
    "    soup = BeautifulSoup(requests.get(WIKI_ROOT + url).text, \"lxml\")\n",
    "    CACHE[url] = soup\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = my_get('/wiki/Listes_des_villes_du_monde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_column_index(row):\n",
    "    header_cols = row.findAll('th')\n",
    "    if len(header_cols) < 1:\n",
    "        header_cols = row.findAll('td')\n",
    "    for idx, header in enumerate(header_cols):\n",
    "        if header.text.lower() in COLUMN_NAMES:\n",
    "            print('index of column with city name is', idx)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cities(country_soup):\n",
    "    tables = country_soup.findAll('table')\n",
    "    cities = []\n",
    "    for table in tables:\n",
    "        rows = table.findAll('tr')\n",
    "        if rows is None:\n",
    "            continue\n",
    "        if len(rows) > ROWS_THRESHOLD:\n",
    "            idx = get_name_column_index(rows[0])\n",
    "            for row in rows[1:]:\n",
    "                city = row.findAll('td')[idx].text\n",
    "                cities.append(city)\n",
    "            break\n",
    "        else:\n",
    "            print('No table with more that {} rows was found in {}'.format(ROWS_THRESHOLD, country_soup.find('h1').text))\n",
    "    return cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_countries(lis):\n",
    "    countries = []\n",
    "    for li in lis:\n",
    "        country = {}\n",
    "        anchor = li.find('a')\n",
    "        country['name'] = anchor.text\n",
    "        country['url'] = anchor['href']\n",
    "        if 'redlink' in country['url']:\n",
    "            print(country['name'], 'does not exist')\n",
    "            continue\n",
    "        print('Scraping', country['name'])\n",
    "        country['cities'] = scrape_cities(BeautifulSoup(my_get(country['url']).text, \"lxml\"))\n",
    "        countries.append(country)\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for header2 in s.findAll(\"h2\"):# class_=\"mw-headline\"):\n",
    "    continent_tag = header2.find('a')\n",
    "    if continent_tag is not None:\n",
    "        continent_name = continent_tag.text\n",
    "        country_list = header2.find_next_sibling().findAll('li')\n",
    "        WORLD[continent_name] = scrape_countries(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('world_scraping.json', 'w') as f:\n",
    "    json.dump(WORLD, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
